#  Mamba 

看上去 `SSM` 就挺好了, 为啥还不行呢, 因为SSM 有两个强假设: `线性` 和 `时不变`, `mamba` 就是对这两个进行改进, 放开了这俩约束

Mamba 的基础是 `HiPPO` 框架，它在数学上通过一种连续正交投影机制，在有限容量的状态向量中保留最重要的历史信息。

所以 SSM 的问题：
> 矩阵不随输入不同而变化，无法针对输入做针对性推理, Linear Time Invariance(LTI)规定 SSM中的A、B、C不随输入不同而不同
> 使得SSM无法针对输入做针对性的推理

```
```
如果是在训练阶段，好理解，毕竟Mamba模型的参数(矩阵B、C和步长Δ)是可学习的，这意味着模型通过梯度下降等优化算法根据输入数据和目标输出调整这些参数

可在推理阶段，模型的参数不应该也是固定的么？

事实上，推理时参数本身还是不变，但由于参数是数据依赖的，模型在推理时可以根据输入数据的特点进行有区别的对待，即对不同的输入token应用不同的B、C和Δ值，换言之，Mamba模型在推理时，可根据不同的输入数据动态计算矩阵B、C和步长Δ的值(推理过程中不会对模型的参数进行重新训练或调整，而是简单地应用训练阶段学到的方式——决定如何计算这些矩阵和步长的函数或映射，来生成预测)
#### 上面的总结:
> 总之，虽然Mamba模型在推理时参数本身也不变，但由于其设计中引入的选择性机制，使得模型能够根据不同输入token的特点进行有区别的对待，这与SSM模型相比是一个显著的进步。且Mamba这种选择性是通过训练阶段的参数学习来实现的(根据训练阶段学习到的参数对不同的输入给予不同的处理)，而不是在推理阶段动态调整参数


> ![](https://image.chiullian.cn/img/202411251546615.png)

- 即这里的不变性特指：推理时不随输入变化而变化，但在训练过程中，矩阵是可以根据需要去做梯度下降而变化的

Mamba 的模型：
* 顺带提前说一嘴，SSM之外，对于mamba，训练时其参数也必然会变
* 但推理时，ssm 不会随着输入的不同 做针对性的推理，即任何输入都是一视同仁，至于参数也不会变
* 但mamba会对输入做选择性推理，虽然推理时本身的参数也不会变，但会对不同的输入给予不同的有区别的对待，比如有的重点关注，有的选择性忽略



> 总之，虽然Mamba模型在推理时参数本身也不变，但由于其设计中引入的选择性机制，使得模型能够根据不同输入token的特点
> 进行有区别的对待，这与SSM模型相比是一个显著的进步。且Mamba这种选择性是通过训练阶段的参数学习来实现的(根据训练阶段学习到的参数对不同的输入给予不同的处理)，而不是在推理阶段动态调整参数

#### S4 模型对信息的重要性聚焦不予处理

没有选择性，S4会花费相同的“精力”来处理每个单词：
![](https://image.chiullian.cn/img/202411251557457.png)

但如果是一个试图对这句话的意图进行分类的模型，它可能会想更多地“关注”order、hamburger，而不是want、to
如下图所示，而通过使模型参数成为输入的函数，模型就可以做到“专注于”输入中对于当前任务更重要的部分，而这正是mamba的创新点之一

![](https://image.chiullian.cn/img/202411251556416.png)

### Mamba 模型的三大创新

整体的架构：
![](https://image.chiullian.cn/img/202411251529329.png)
简述： 
> (总开关加了几个非线性的电子小开关)(也就是 delta t), 总开光是非线性的,那么这几个小开关(控制A,控制B, 控制C的开关)控制的也是非线性的, 他是跟输入x相关的,所以是时变的, 所以由小开关控制的A,B,C也就是非线性时变的
> 
> 解释一下蓝色部分,也就是对输入X的压缩投影的学习的可变参数, 要是我只知道delta t我怎么要知道选择什么,处理什么，所以通过输入的信息才能知道这些开关是怎么联动的怎么聚合的
> 
> 最重要的一点就是加了一个 (delta t) 也就是开关, 加了非线性的激活函数, 所以由他控制的就是非线性的,

对时间步长的理解:

![](https://image.chiullian.cn/img/202411252104982.png)

> Mamba = 有选择处理信息 + 硬件感知算法 + 更简单的SSM架构

`Transformer` 就像人类每写一个字之前，都把前面的所有字+输入都复习一遍，所以写的慢
`RNN` 每次只参考前面固定的字数,写的快是快，但容易忘掉更前面的内容
`SSM` 的问题在于其中的矩阵A B C不随输入不同而不同，即无法针对不同的输入针对性的推理

> Mamba的解决办法是，相比SSM压缩所有历史记录，mamba设计了一个简单的选择机制，通过“参数化SSM的输入”，让模型对信息有选择性处理，以便关注或忽略特定的输入

#### 如何有效的选择的：
> Mamba每次参考前面所有内容的一个概括，越往后写对前面内容概括得越狠，丢掉细节、保留大意

两种能力 `抓重点能力` `上下文推理能力`

怎么增加的选择性：

![](https://image.chiullian.cn/img/202411252014084.png)

参数解释:
![](https://image.chiullian.cn/img/202411252028538.png)

#### 由静态 A B C 变动态 A B C

![](https://image.chiullian.cn/img/202411281616833.png)

然后进行离散化:

![](https://image.chiullian.cn/img/202411281618290.png)

对选择函数进行解释

![](https://image.chiullian.cn/img/202411252058459.png)

LSTM 类似于一本日记的摘要，直接记录了哪些重要事件发生过，但不会详细描述事件的动态过程。
SSM 更像是一段连续的视频轨迹，记录了事情是如何发生和演化的。

LSTM 更倾向于在每个时间步更新一个静态状态 ht，保留过去的核心特征。
SSM 的状态 h(t) 实际是通过动态投影来记录历史的“瞬时动态”（例如，HiPPO 中的正交系数表示）。

LSTM 关注的是“我记住了什么”，以输出的累积状态为核心。
SSM 关注的是“事情是如何变化的”，用变化轨迹刻画历史。

> 使用微分方程和状态转移矩阵来建模序列数据的变化。HiPPO 是一种特别的 SSM，它通过正交多项式（如 Legendre 基）系数来近似历史信息。

#### Lstm 与 Mamba 记录历史信息的不同地方
LSTM 用固定维度的记忆单元 ct 累积历史信息，但信息可能因为训练中的梯度消失而丢失。
SSM 用状态向量 h(t)和正交基（如 Legendre）来投影和逼近历史信息，数学上有严格的近似保证。

![](https://image.chiullian.cn/img/202411252144156.png)
还有一个重要的是SSM是可以并行的,而lstm不行

#### 为什么 A 不加时间变换t ?

![](https://image.chiullian.cn/img/202411261540046.png)

> 首先A毋庸置疑是记录浓缩了历史信息的一个矩阵, 想当于控制着历史历史的主线(全局演变路径), 所以不带时间t是非常好的,


##### 与Transformer 的区别

![](https://image.chiullian.cn/img/202411261543700.png)

#### Mamba 具体架构

![](https://image.chiullian.cn/img/202411261553103.png)


#### Mamba = GRU + CNN + 选择性注意力机制

### Mamba 总结
#### 如何压缩历史信息的 ?

> HiPPO 和状态空间模型

![](https://image.chiullian.cn/img/202411261626581.png)

#### 
![](https://image.chiullian.cn/img/202411261706466.png)

### 着重回顾一下对历史信息的处理问题：

![](https://image.chiullian.cn/img/202411261715117.png)

`A` 反映了时间维度的选择性记忆；能对不同时间点的历史信息动态加权。
`B` 反映了输入特征的重要性。

#### 非常好的问题

对于不同维度他选取的时间步长还是不同的

![](https://image.chiullian.cn/img/202411272226772.png)

如何获取每个维度的步长, 也解释了加映射的作用

![](https://image.chiullian.cn/img/202411272228189.png)

![](https://image.chiullian.cn/img/202411271953284.png)

#### 还有一个重要的问题 为什么 步长从 delta t 变成了一个矩阵

1. 为什么 Mamba 模型中的步长 Δ 会是一个多维矩阵，而不是一个简单的标量？

![](https://image.chiullian.cn/img/202411272208983.png)

![](https://image.chiullian.cn/img/202411272217807.png)


### Mamba 与 SSM 的具体处理区别表
> 当在 Mamba1 中被引入为选择性 SSM时，则相当于允许(A, B, C)这三个参数随时间而变化

在Mamaba中，作者让B矩阵、C矩阵、\Delta成为输入的函数，让模型能够根据输入内容自适应地调整其行为

![](https://image.chiullian.cn/img/202411281248573.png)

### 具体如何处理的, 如何嵌入

![](https://image.chiullian.cn/img/202411281630389.png)

#### 为什么 SSM 前面有个卷积?
本质是对数据做进一步的预处理，更细节的原因在于:
1. SSM之前的CNN负责提取局部特征(因其擅长捕捉局部的短距离特征)，而SSM则负责处理这些特征并捕捉序列数据中的长期依赖关系，两者算互为补充
2. CNN有助于建立token之间的局部上下文关系，从而防止独立的token计算

### 如何并行的?

![](https://image.chiullian.cn/img/202411302035274.png)