# Mamba2

> 论文主要强调的对偶关系, 
> SSM 模型主要强调的是顺序和逻辑, 而 Transformer强调的是关系和关联, 文章就是强调他俩的关系的对偶, 然后是他俩有效的结合,达成完美的效果

#### SSM 矩阵的巧妙设计

> 尽管 Mamba 使用的是并行的, 但是还是需要特殊的硬件设备才能加速, 即便如此效率还是不如硬件友好的模型(如 CNN 和 Transformer)高效
> 因为它没有利用***矩阵乘法***单元，而现代加速器(如 GPU 和 TPU)正是为此而专门设计的

#### 这是Mamba1的 步骤

![](https://image.chiullian.cn/img/202411282236051.png)


总之，虽然时间不变SSM 与连续、递归和卷积序列模型密切相关，但它们与注意力机制没有直接关系。所以mamba2想揭示选择性SSM和注意力机制之间的更深层次关系，并利用这一点显著提高SSM的训练速度，同时允许更大的状态规模N

重要定义:
![](https://image.chiullian.cn/img/202411281510677.png)

#### Mamba1 的并行缺陷: !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
> 时间复杂度为 O(N / t), t为核的数量

![](https://image.chiullian.cn/img/202411282240071.png)

更加详细点就是:
> 因为从mid中间分开(非常非常像 线段树的合并), 左右是独立的. 
> 
> H3 = AH2 + BX3, 此时算出来的 H2是未知的但是除了未知的, 剩下的都算出来了, H2会在左子树里算完, 然后合并就是把值接过来算
> ![](https://image.chiullian.cn/img/202411282304250.png)
![](https://image.chiullian.cn/img/202411282302378.png)

用来优化的并行扫描的

> 归根结底我们的最终目的还是***去算一个 y = Mx 的M矩阵***, 而Mamba1 并行显然不够, 所以Mamba就是对他做了进一步的优化, 改成使用矩阵操作

![](https://image.chiullian.cn/img/202411301640540.png)

#### 引入半可分矩阵

首先，先来看下半可分矩阵(Semiseparable Matrices)的定义「称之为定义3.1，在有的文献中也被称为 (N, 0)-半可分性)」
一个（下三角）矩阵 𝑀是 N-半可分的，如果包含在下三角部分(即对角线或以下)的每个子矩阵的秩——Rank最多为 N，则称 N为半可分矩阵的阶数或秩



