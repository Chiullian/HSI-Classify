# Mamba2

> 论文主要强调的对偶关系, 
> SSM 模型主要强调的是顺序和逻辑, 而 Transformer强调的是关系和关联, 文章就是强调他俩的关系的对偶, 然后是他俩有效的结合,达成完美的效果

#### SSM 矩阵的巧妙设计

> 尽管 Mamba 使用的是并行的, 但是还是需要特殊的硬件设备才能加速, 即便如此效率还是不如硬件友好的模型(如 CNN 和 Transformer)高效
> 因为它没有利用***矩阵乘法***单元，而现代加速器(如 GPU 和 TPU)正是为此而专门设计的

#### 这是Mamba1的 步骤

![](https://image.chiullian.cn/img/202411282236051.png)


总之，虽然时间不变SSM 与连续、递归和卷积序列模型密切相关，但它们与注意力机制没有直接关系。所以mamba2想揭示选择性SSM和注意力机制之间的更深层次关系，并利用这一点显著提高SSM的训练速度，同时允许更大的状态规模N

重要定义:
![](https://image.chiullian.cn/img/202411281510677.png)

#### Mamba1 的并行缺陷: !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
> 时间复杂度为 O(N / t), t为核的数量

![](https://image.chiullian.cn/img/202411282240071.png)

更加详细点就是:
> 因为从mid中间分开(非常非常像 线段树的合并), 左右是独立的. 
> 
> H3 = AH2 + BX3, 此时算出来的 H2是未知的但是除了未知的, 剩下的都算出来了, H2会在左子树里算完, 然后合并就是把值接过来算
> ![](https://image.chiullian.cn/img/202411282304250.png)
![](https://image.chiullian.cn/img/202411282302378.png)

用来优化的并行扫描的

> 归根结底我们的最终目的还是***去算一个 y = Mx 的M矩阵***, 而Mamba1 并行显然不够, 所以Mamba就是对他做了进一步的优化, 改成使用矩阵操作

![](https://image.chiullian.cn/img/202411301640540.png)

#### 引入半可分矩阵
> 简单介绍一下`结构化矩阵`, 其实就是为了简化矩阵乘法, 让矩阵可以压缩, 用更少的参数表示一个矩阵, 并且通过快速算法(最重要的是矩阵乘法)，直接操作这种压缩表示


首先，先来看下半可分矩阵(Semiseparable Matrices)的定义「称之为定义3.1，在有的文献中也被称为 (N, 0)-半可分性)」
一个（下三角）矩阵 𝑀是 N-半可分的，如果包含在下三角部分(即对角线或以下)的每个子矩阵的秩——Rank最多为 N，则称 N为半可分矩阵的阶数或秩

> 简单来讲半可分矩阵就是 M 矩阵, 半可分矩阵(一个下三角矩阵), 而且每个浅紫色的小框里的秩很小
> ![](https://image.chiullian.cn/img/202412022240155.png)


## 最重要的点 关于 Mamba2

本文最重要的短就是设计了一个这个矩阵:
![](https://image.chiullian.cn/img/202412031409137.png)

大家不要被如下矩阵给误扰了, 这个式子只是表明了 M 的矩阵设计就相当于 QK^T * V 而实际的 表示了 M 矩阵里就是注意力机制:
![](https://image.chiullian.cn/img/202412031410401.png)

> 在经典的注意力机制中，QK⊤ 是核心部分，用于计算所有时间步之间的相似性，复杂度为 O(n^2)。
> 在 Mamba-2 中，QK⊤ 的显式形式确实 ***不存在***，它的功能通过状态空间模型中 M 的构造隐式实现。

M 矩阵中的每一项![](https://image.chiullian.cn/img/202412031418859.png) 其实就是注意力:

> 其中每个元素 Mij 直接表示两个时间步 Xi,Xj 之间的隐式相似性（即注意力权重）。 这意味着：Mij 就是注意力权重，用于衡量时间步 Xi 和 Xj 的相互影响。

### !!! Mamba中 是如何进行注意力的操作的:
> Cj 和 Bi 是列向量, 分别代表行和列的权重


![](https://image.chiullian.cn/img/202412031425213.png)


对比传统注意力的详细的区别对比:
![](https://image.chiullian.cn/img/202412031422605.png)

> 有个比较重要的点就是 Hippo 即压缩后的矩阵对角线是固定的(都是一个相同的数)


当秩为 1 的时候上面的半可分矩阵会退化SSM矩阵

> 因此，也将1-SS矩阵的矩阵乘法称为标量SSM递归或累积乘积和(累积乘积和的广义形式)作为递归的基本形式，同时也是本次mamba2主要算法的构建模块
> 也从侧面说明，许多序列模型的算法可以归结为结构化矩阵乘法算法


