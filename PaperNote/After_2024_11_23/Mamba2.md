# Mamba2

### 前置知识:
#### 矩阵中特征值的作用和特征向量的作用
* 特征值的作用: 特征值描述了一个矩阵（或线性变换）在某些特殊方向上的缩放因子，它揭示了矩阵的重要特性。
* 特征向量描述了矩阵（或线性变换）在特定方向上的不变性方向，是线性变换中被保持形状的“特定方向”。

![](https://image.chiullian.cn/img/202412051411385.png)
在 Mamba1 中，如果 A 是对角阵，并且应用目标仅涉及快速分解和计算，则可以忽略特征向量，仅保留特征值对角矩阵。 
但如果后续步骤需要用特征向量描述方向性或模式，则仍需保留。若您对具体部分有疑问，可以提供更多细节以确认。

这里终于说通了:
Mamba2 为了使用矩阵运算并加速, 进一步的简单化 A矩阵, 从原来的对角阵(Mamba1)约束上再增加了约束,对角线的值一样,这样就变成了`A = I*a` a是一个标量

#### 博客还解释了,为何单独的标量不会有损害呢:

> * 首先 Mamba1 的每个头也可以理解为每个通道, 而Mamba1的A是对角矩阵, 所以相当于,每个值控制一个通道 (***即所有通道完全由独立的 SSMs 独立控制***)
> * 然而 Mamba2 中的 是所有通道共享一个权重, 这样就可以加大状态维度 N

 -[x] 具体是会不会损坏信息, 作者说还不能证明,但是一个很好的研究方向

#### 博客为此举了一个很形象的例子:
在Mamba中引入选择性（例如，A取决于输入X）的主要原因之一是让SSM能够控制是否记住或忽略特定的信息片段;
例如，如果在文本转录中遇到填充符“um”。但是如果这些信息应该被忽略，那么整个状态可以一起忽略它，
所以如果状态的动态在所有特征之间共享，应该是可以的。

#### 如何解决的注意力的N^2的问题呢.

L 矩阵定义如下:

![](https://image.chiullian.cn/img/202412051525641.png)

M 的矩阵定义如下:
![](https://image.chiullian.cn/img/202412051527211.png)

其实 M 就相当于注意力机制了 C 左拉出来, B右拉出来, A又是一个标量*单位矩阵, 就变成了L:
![](https://image.chiullian.cn/img/202412051526800.png)
其可以被解释为`基于位置i和j相距多远的“折扣因子”`。
(This Tobias Katsch的GateLoop论文同时支持这种解释。在它的注意力形式中，这个依赖于输入的位置掩码可以被解释为编码曼巴的“选择性”的关键因素！



具体解释如下:

![](https://image.chiullian.cn/img/202412051511386.png)

> 简单来说就是:单个 SSM 头的总状态大小为，在 Mamba1 中每个都由单独的标量递归控制，但在 Mamba2 中由单个共享递归控制。

觉得4回答的很有说服力

> 然后就是用一些证明手法证明了注意力就是特殊的状态对偶模型

```
特定应用的合理性
在某些特定的神经网络架构中（例如 Mamba-2 的 SSD 层），这样简化的矩阵 A 或许足以满足任务需求。
特别是，如果网络的其他部分已经承担了大部分的复杂非线性变换
那么 A 的主要作用可能仅仅是提供一个全局的缩放因子。
```
![](https://image.chiullian.cn/img/202412051420759.png)


![](https://image.chiullian.cn/img/202412051426207.png)
上图的第一个表示的是SSM4,第二个是Mamba1里的, 而第三个只有标量的表示SSM6

> Mamba2的主要目的是使用矩阵乘法,充分利用GPU

#### SSM 矩阵的巧妙设计

> 论文主要强调的对偶关系(`注意力和SSM上升都属于同一个根的东西`), 
> SSM 模型主要强调的是顺序和逻辑, 而 Transformer强调的是关系和关联, 文章就是强调他俩的关系的对偶, 然后是他俩有效的结合,达成完美的效果

> 尽管 Mamba 使用的是并行的, 但是还是需要特殊的硬件设备才能加速, 即便如此效率还是不如硬件友好的模型(如 CNN 和 Transformer)高效
> 因为它没有利用***矩阵乘法***单元，而现代加速器(如 GPU 和 TPU)正是为此而专门设计的

#### 这是Mamba1的 步骤

![](https://image.chiullian.cn/img/202411282236051.png)


总之，虽然时间不变SSM 与连续、递归和卷积序列模型密切相关，但它们与注意力机制没有直接关系。所以mamba2想揭示选择性SSM和注意力机制之间的更深层次关系，并利用这一点显著提高SSM的训练速度，同时允许更大的状态规模N

重要定义:
![](https://image.chiullian.cn/img/202411281510677.png)

#### Mamba1 的并行缺陷: !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
> 时间复杂度为 O(N / t), t为核的数量

![](https://image.chiullian.cn/img/202411282240071.png)

更加详细点就是:
> 因为从mid中间分开(非常非常像 线段树的合并), 左右是独立的. 
> 
> H3 = AH2 + BX3, 此时算出来的 H2是未知的但是除了未知的, 剩下的都算出来了, H2会在左子树里算完, 然后合并就是把值接过来算
> ![](https://image.chiullian.cn/img/202411282304250.png)
![](https://image.chiullian.cn/img/202411282302378.png)

用来优化的并行扫描的

> 归根结底我们的最终目的还是***去算一个 y = Mx 的M矩阵***, 而Mamba1 并行显然不够, 所以Mamba就是对他做了进一步的优化, 改成使用矩阵操作

![](https://image.chiullian.cn/img/202411301640540.png)

#### 引入半可分矩阵
> 简单介绍一下`结构化矩阵`, 其实就是为了简化矩阵乘法, 让矩阵可以压缩, 用更少的参数表示一个矩阵, 并且通过快速算法(最重要的是矩阵乘法)，直接操作这种压缩表示


首先，先来看下半可分矩阵(Semiseparable Matrices)的定义「称之为定义3.1，在有的文献中也被称为 (N, 0)-半可分性)」
一个（下三角）矩阵 𝑀是 N-半可分的，如果包含在下三角部分(即对角线或以下)的每个子矩阵的秩——Rank最多为 N，则称 N为半可分矩阵的阶数或秩

> 简单来讲半可分矩阵就是 M 矩阵, 半可分矩阵(一个下三角矩阵), 而且每个浅紫色的小框里的秩很小
> ![](https://image.chiullian.cn/img/202412022240155.png)


## 最重要的点 关于 Mamba2

本文最重要的短就是设计了一个这个矩阵:
![](https://image.chiullian.cn/img/202412031409137.png)

大家不要被如下矩阵给误扰了, 这个式子只是表明了 M 的矩阵设计就相当于 QK^T * V 而实际的 表示了 M 矩阵里就是注意力机制:
![](https://image.chiullian.cn/img/202412031410401.png)

> 在经典的注意力机制中，QK⊤ 是核心部分，用于计算所有时间步之间的相似性，复杂度为 O(n^2)。
> 在 Mamba-2 中，QK⊤ 的显式形式确实 ***不存在***，它的功能通过状态空间模型中 M 的构造隐式实现。

M 矩阵中的每一项![](https://image.chiullian.cn/img/202412031418859.png) 其实就是注意力:

> 其中每个元素 Mij 直接表示两个时间步 Xi,Xj 之间的隐式相似性（即注意力权重）。 这意味着：Mij 就是注意力权重，用于衡量时间步 Xi 和 Xj 的相互影响。

### !!! Mamba中 是如何进行注意力的操作的:
> Cj 和 Bi 是列向量, 分别代表行和列的权重


![](https://image.chiullian.cn/img/202412031425213.png)


对比传统注意力的详细的区别对比:
![](https://image.chiullian.cn/img/202412031422605.png)

> 有个比较重要的点就是 Hippo 即压缩后的矩阵对角线是固定的(都是一个相同的数)


当秩为 1 的时候上面的半可分矩阵会退化SSM矩阵

> 因此，也将1-SS矩阵的矩阵乘法称为标量SSM递归或累积乘积和(累积乘积和的广义形式)作为递归的基本形式，同时也是本次mamba2主要算法的构建模块
> 也从侧面说明，许多序列模型的算法可以归结为结构化矩阵乘法算法

#### 很显然我们的目的是更快的计算出 中间一对矩阵的乘法 Aj...i
> 因为 C 矩阵和 B矩阵都是一维的, 先把中间的算出来能加速运算, 所以矩阵A的设计非常重要也就是 Hippo 矩阵


![](https://image.chiullian.cn/img/202412041606753.png)

### 如何增加的计算效率，具体怎么计算的

一般总结成如下图：
![](https://image.chiullian.cn/img/202412051533914.png)
