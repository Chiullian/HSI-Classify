{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### PCA 主成分分析\n",
    "> PCA（Principal Component Analysis） 是一种常见的数据分析方式，常用于高维数据的降维，可用于提取数据的主要特征分量。\n",
    "\n",
    "easy 来讲就是对于一堆数据投射到一条直线(平面或更高维) 的点, 使得各个点之间的方差最大 (可以这样认为, 协方差越大, 投射后所保留的信息就越多)\n",
    "\n",
    "在一维空间中我们可以用方差来表示数据的分散程度。而对于高维数据，我们用协方差进行约束，协方差可以表示两个变量的相关性。为了让两个变量尽可能表示更多的原始信息，我们希望它们之间不存在线性相关性，因为相关性意味着两个变量不是完全独立，必然存在重复表示的信息。\n",
    "\n",
    "看到这张图片就会豁然开朗:\n",
    "\n",
    "![](https://image.chiullian.cn/img/202410081656948.png)\n",
    "> 用了更少的点保留了更多的信息\n",
    "\n",
    "协方差矩阵\n",
    "> 协方差表示的是两个变量在变化的时候是同方向变化的还是反向变化的, 以及同方向和反方向的程度如何.\n",
    "\n",
    "![](https://image.chiullian.cn/img/202410081700162.png)\n",
    "\n",
    "**协方差矩阵的特征向量就是坐标的偏转方向R矩阵**\n",
    "\n",
    "整体流程如下:\n",
    "\n",
    "![](https://image.chiullian.cn/img/202410081703688.png)\n"
   ],
   "id": "25af3ef75b8f3554"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "代码(直接调包):\n",
    "PCA 参数\n",
    "* n_components: 要把原始数据压缩成几维度\n",
    "* copy: True 或False，默认为True，即是否需要将原始训练数据复制。\n",
    "* whiten：True 或False，默认为False，即是否白化，使得每个特征具有相同的方差。\n",
    "\n",
    "fit(X):\t用数据X来训练PCA模型。\n",
    "fit_transform(X)\t用X来训练PCA模型，同时返回降维后的数据。\n",
    "inverse_transform(newData)\tnewData 为降维后的数据。将降维后的数据转换成原始数据，但可能不会完全一样，会有些许差别。\n",
    "\n",
    "`这里要注意的是 压缩的维度要 <= Batch_size 且 <= 特征维度长度 即 <= min(n_samples, n_features)` "
   ],
   "id": "71ba1f5b675a75e7"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-08T10:32:12.817609Z",
     "start_time": "2024-10-08T10:32:12.811544Z"
    }
   },
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from einops import rearrange\n",
    "import torch\n",
    "\n",
    "s = 6\n",
    "X = torch.arange(50 * 3 * 20).reshape(40, 3, 5, 5)\n",
    "X = rearrange(X, 'b c h w -> b (c h w)')\n",
    "print('Before PCA, X shape: ', X.shape)\n",
    "# print(X)\n",
    "pca = PCA(s, whiten=True)\n",
    "X = pca.fit_transform(X)\n",
    "print(\"After PCA, X shape: \", X.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before PCA, X shape:  torch.Size([40, 75])\n",
      "After PCA, X shape:  (40, 6)\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## K-means\n",
    "\n",
    "> 我们不知道数据的标签, 我们只在数据中去找去相似的数据组\n",
    "\n",
    "具体步骤:\n",
    "每一轮进行如下步骤\n",
    "1. 第一步先从原数据中随机找出k个数据点, 当做质心\n",
    "2. 每一个点计算到k个质心(k个类里面的中心点)的距离, 然后离得近的就属于哪一类\n",
    "3. 根据k簇类的所有点计算平均值, 得到新的质心点\n",
    "\n",
    "that is all ...\n"
   ],
   "id": "cbbaa51c24cc27d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from sklearn.cluster import KMeans",
   "id": "5c17921a1a2aca31"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
