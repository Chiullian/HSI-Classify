{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Vision Transformer ( 简称vits)\n",
    "![](https://image.chiullian.cn/img/202410071124188.png)\n",
    "\n",
    "整体框架大致如下\n",
    "![](https://image.chiullian.cn/img/202410071106925.png)\n",
    "由于transformer 是seq2seq的模型, 输入都是基于一个序列的\n",
    "所以首先第一思想是把每个像素按照顺序排好序, 然后当做输入部分, 但是要考虑到的是一个 图片的像素量太大了很难处理, 所以这里考虑切割图片例如, 把一个图片切割成 16*16 的大小的, 当成一个transformer 里的一个token(单词), 然后 Embedding 一下\n",
    "![](https://image.chiullian.cn/img/202410071105835.png)\n",
    "1. 该模型只考虑使用 transformer 的 encoder 部分\n",
    "\n",
    "![](https://image.chiullian.cn/img/202410071123977.png)\n",
    "\n"
   ],
   "id": "bc26e78375ad4332"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-01T03:18:40.493587Z",
     "start_time": "2024-12-01T03:18:34.954776Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.Layernorm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.Layernorm(x), **kwargs)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, n_heads, dim_head, dropout):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * n_heads\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3)\n",
    "        \n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        \"用于消除误差，保证方差为1，避免向量内积过大导致的softmax将许多输出置0的情况\"\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"注意到这里的X形状为 b * n * dim\"\"\"\n",
    "        qkv = self.to_qkv(X).chunk(3, dim=-1)\n",
    "        \"\"\"qkv的形状为b * n * 3 * inner_dim, 这也就相当于平分原来一个大矩阵当作q, k, v\"\"\"\n",
    "        Q, K, V = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.n_heads), qkv)\n",
    "        \"\"\"类似于transformer里的把多头和batch_size放在一起， 这样就可以只用一次矩阵运算\"\"\"\n",
    "        dots = torch.matmul(Q, K.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        out = torch.matmul(attn, V)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.net(X)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    包含两大部分\n",
    "    1. 多头注意力机制部分\n",
    "    2. 前馈神经网络部分\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, n_layers, n_heads, dim_head, mlp_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(n_layers):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, n_heads=n_heads, dim_head=dim_head, dropout=dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "\n",
    "class ViT(torch.nn.Module):\n",
    "    \"\"\"\"\n",
    "        参数解释大全\n",
    "        image_size: (h, w) 图片的高宽\n",
    "        patch_size: (h, w) 切成小块的大小\n",
    "        dim: 相当于 transformer 里的 d_models， 固定词向量的长度\n",
    "        n_classes: 分类任务的分类数量\n",
    "        n_layers: 迭代块的数量\n",
    "        n_heads: 多头的数量\n",
    "        dim_head: 每一个头的维度是多少\n",
    "        mlp_dim: 前馈神经网络的隐藏维度的大小\n",
    "        channels: 图片通道的数量\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size, patch_size, dim, n_classes, n_layers, n_heads, mlp_dim, pool='cls', channels=3,\n",
    "                 dim_head=64, dropout=0.1, emb_dropout=0.1):\n",
    "        super(ViT, self).__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "        \"必须能把图片正好分割\"\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0\n",
    "\n",
    "        \"一个大图片能分成多少个块\"\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        \"一个块能能拉成多长\"\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "        \"一种优雅的方式变换向量的维度\"\n",
    "        self.to_path_emb = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_height, p2=patch_width),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, n_layers, n_heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, image):\n",
    "        X = self.to_path_emb(image)\n",
    "        b, n, _ = X.shape  # X 的形状为 batch * n * dim\n",
    "        cls_token = repeat(self.cls_token, '() n d -> b n d', b=b)  # 形状为 batch * 1 * dim\n",
    "        \"头部添加一个 cls 的 token 引用 bert 的技巧, 该cls可以同时获取全局的信息(由于自注意力机制)\"\n",
    "        X = torch.cat((cls_token, X), dim=1)\n",
    "        \"利用广播机制把 每一个n加上位置信息\"\n",
    "        X += self.pos_embedding[:, :(n + 1)]\n",
    "\n",
    "        X = self.dropout(X)\n",
    "        X = self.transformer(X)\n",
    "        \"其实不加cls也行也就是最后池化的时候取个平均， 两种方法\"\n",
    "        X = X.mean(dim=1) if self.pool == 'mean' else X[:, 0]  # b n + 1 dim 只取第第一个 b 1 dim, 第一个包含了所有的信息!\n",
    "        return self.mlp_head(X)\n",
    "\n",
    "\n",
    "v = ViT(\n",
    "    image_size=224,\n",
    "    patch_size=16,\n",
    "    n_classes=1000,\n",
    "    dim=1024,\n",
    "    n_layers=6,\n",
    "    n_heads=16,\n",
    "    mlp_dim=2048,\n",
    "    dropout=0.1,\n",
    "    emb_dropout=0.1\n",
    ")\n",
    "\n",
    "img = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "preds = v(img)  # (1, 1000)\n",
    "print(max(nn.Softmax()(preds)))\n",
    "print(preds.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0022, 0.0016, 0.0005, 0.0008, 0.0021, 0.0012, 0.0006, 0.0012, 0.0007,\n",
      "        0.0006, 0.0034, 0.0014, 0.0006, 0.0009, 0.0016, 0.0017, 0.0011, 0.0018,\n",
      "        0.0007, 0.0006, 0.0018, 0.0009, 0.0006, 0.0009, 0.0016, 0.0008, 0.0008,\n",
      "        0.0010, 0.0013, 0.0005, 0.0006, 0.0031, 0.0007, 0.0024, 0.0016, 0.0009,\n",
      "        0.0013, 0.0007, 0.0002, 0.0004, 0.0011, 0.0015, 0.0017, 0.0008, 0.0010,\n",
      "        0.0005, 0.0009, 0.0010, 0.0006, 0.0014, 0.0016, 0.0009, 0.0007, 0.0012,\n",
      "        0.0014, 0.0013, 0.0004, 0.0007, 0.0005, 0.0015, 0.0010, 0.0008, 0.0009,\n",
      "        0.0007, 0.0006, 0.0006, 0.0008, 0.0006, 0.0023, 0.0013, 0.0006, 0.0017,\n",
      "        0.0016, 0.0005, 0.0009, 0.0005, 0.0005, 0.0013, 0.0009, 0.0015, 0.0006,\n",
      "        0.0007, 0.0013, 0.0022, 0.0003, 0.0014, 0.0007, 0.0004, 0.0015, 0.0013,\n",
      "        0.0006, 0.0008, 0.0008, 0.0010, 0.0005, 0.0008, 0.0005, 0.0013, 0.0010,\n",
      "        0.0031, 0.0017, 0.0009, 0.0006, 0.0016, 0.0014, 0.0010, 0.0010, 0.0006,\n",
      "        0.0011, 0.0013, 0.0005, 0.0003, 0.0018, 0.0008, 0.0008, 0.0013, 0.0009,\n",
      "        0.0006, 0.0008, 0.0013, 0.0005, 0.0014, 0.0005, 0.0004, 0.0009, 0.0010,\n",
      "        0.0020, 0.0008, 0.0008, 0.0010, 0.0016, 0.0013, 0.0012, 0.0010, 0.0005,\n",
      "        0.0007, 0.0015, 0.0009, 0.0005, 0.0010, 0.0011, 0.0010, 0.0019, 0.0004,\n",
      "        0.0007, 0.0006, 0.0014, 0.0011, 0.0023, 0.0005, 0.0017, 0.0005, 0.0009,\n",
      "        0.0007, 0.0007, 0.0008, 0.0008, 0.0003, 0.0004, 0.0007, 0.0006, 0.0005,\n",
      "        0.0015, 0.0006, 0.0007, 0.0016, 0.0010, 0.0012, 0.0005, 0.0014, 0.0011,\n",
      "        0.0005, 0.0011, 0.0007, 0.0004, 0.0004, 0.0019, 0.0006, 0.0009, 0.0008,\n",
      "        0.0008, 0.0007, 0.0011, 0.0010, 0.0011, 0.0008, 0.0009, 0.0003, 0.0009,\n",
      "        0.0010, 0.0019, 0.0005, 0.0003, 0.0011, 0.0008, 0.0016, 0.0011, 0.0009,\n",
      "        0.0005, 0.0008, 0.0006, 0.0014, 0.0003, 0.0024, 0.0005, 0.0007, 0.0021,\n",
      "        0.0012, 0.0014, 0.0029, 0.0006, 0.0004, 0.0011, 0.0005, 0.0011, 0.0009,\n",
      "        0.0008, 0.0003, 0.0012, 0.0020, 0.0012, 0.0010, 0.0003, 0.0018, 0.0004,\n",
      "        0.0004, 0.0007, 0.0011, 0.0010, 0.0015, 0.0007, 0.0026, 0.0010, 0.0015,\n",
      "        0.0012, 0.0006, 0.0004, 0.0003, 0.0010, 0.0016, 0.0010, 0.0020, 0.0015,\n",
      "        0.0004, 0.0016, 0.0009, 0.0012, 0.0013, 0.0009, 0.0003, 0.0016, 0.0006,\n",
      "        0.0007, 0.0016, 0.0009, 0.0005, 0.0008, 0.0004, 0.0002, 0.0022, 0.0012,\n",
      "        0.0008, 0.0033, 0.0005, 0.0009, 0.0005, 0.0018, 0.0016, 0.0011, 0.0007,\n",
      "        0.0013, 0.0010, 0.0003, 0.0015, 0.0004, 0.0008, 0.0040, 0.0013, 0.0006,\n",
      "        0.0006, 0.0014, 0.0007, 0.0006, 0.0013, 0.0024, 0.0020, 0.0006, 0.0005,\n",
      "        0.0003, 0.0010, 0.0007, 0.0004, 0.0006, 0.0004, 0.0007, 0.0004, 0.0005,\n",
      "        0.0021, 0.0002, 0.0006, 0.0010, 0.0004, 0.0028, 0.0004, 0.0010, 0.0007,\n",
      "        0.0007, 0.0021, 0.0010, 0.0006, 0.0003, 0.0011, 0.0007, 0.0007, 0.0014,\n",
      "        0.0015, 0.0010, 0.0007, 0.0013, 0.0011, 0.0007, 0.0009, 0.0008, 0.0011,\n",
      "        0.0005, 0.0019, 0.0004, 0.0008, 0.0008, 0.0002, 0.0005, 0.0009, 0.0010,\n",
      "        0.0002, 0.0008, 0.0016, 0.0017, 0.0010, 0.0012, 0.0007, 0.0006, 0.0005,\n",
      "        0.0016, 0.0008, 0.0030, 0.0005, 0.0011, 0.0021, 0.0019, 0.0012, 0.0008,\n",
      "        0.0007, 0.0014, 0.0013, 0.0007, 0.0004, 0.0008, 0.0011, 0.0009, 0.0010,\n",
      "        0.0007, 0.0011, 0.0004, 0.0011, 0.0011, 0.0022, 0.0006, 0.0011, 0.0004,\n",
      "        0.0009, 0.0001, 0.0007, 0.0010, 0.0008, 0.0011, 0.0007, 0.0011, 0.0008,\n",
      "        0.0003, 0.0005, 0.0005, 0.0019, 0.0004, 0.0010, 0.0015, 0.0017, 0.0018,\n",
      "        0.0005, 0.0001, 0.0020, 0.0011, 0.0006, 0.0005, 0.0003, 0.0003, 0.0012,\n",
      "        0.0006, 0.0004, 0.0019, 0.0006, 0.0004, 0.0009, 0.0007, 0.0009, 0.0005,\n",
      "        0.0003, 0.0007, 0.0007, 0.0011, 0.0011, 0.0004, 0.0007, 0.0001, 0.0017,\n",
      "        0.0004, 0.0022, 0.0015, 0.0008, 0.0006, 0.0011, 0.0003, 0.0011, 0.0009,\n",
      "        0.0005, 0.0007, 0.0014, 0.0019, 0.0017, 0.0004, 0.0030, 0.0010, 0.0014,\n",
      "        0.0005, 0.0014, 0.0008, 0.0005, 0.0017, 0.0011, 0.0006, 0.0012, 0.0007,\n",
      "        0.0015, 0.0009, 0.0009, 0.0006, 0.0001, 0.0014, 0.0003, 0.0006, 0.0005,\n",
      "        0.0014, 0.0028, 0.0018, 0.0004, 0.0009, 0.0002, 0.0017, 0.0004, 0.0003,\n",
      "        0.0013, 0.0006, 0.0023, 0.0010, 0.0012, 0.0004, 0.0015, 0.0006, 0.0007,\n",
      "        0.0027, 0.0003, 0.0014, 0.0008, 0.0004, 0.0022, 0.0006, 0.0009, 0.0005,\n",
      "        0.0008, 0.0007, 0.0009, 0.0007, 0.0003, 0.0005, 0.0010, 0.0010, 0.0016,\n",
      "        0.0010, 0.0007, 0.0012, 0.0007, 0.0008, 0.0020, 0.0009, 0.0024, 0.0009,\n",
      "        0.0006, 0.0011, 0.0004, 0.0011, 0.0004, 0.0010, 0.0015, 0.0022, 0.0011,\n",
      "        0.0013, 0.0009, 0.0010, 0.0009, 0.0008, 0.0010, 0.0016, 0.0008, 0.0005,\n",
      "        0.0005, 0.0017, 0.0005, 0.0009, 0.0005, 0.0006, 0.0013, 0.0011, 0.0007,\n",
      "        0.0004, 0.0006, 0.0010, 0.0015, 0.0004, 0.0015, 0.0005, 0.0007, 0.0008,\n",
      "        0.0017, 0.0021, 0.0003, 0.0005, 0.0008, 0.0008, 0.0003, 0.0011, 0.0007,\n",
      "        0.0005, 0.0007, 0.0008, 0.0004, 0.0007, 0.0005, 0.0010, 0.0006, 0.0011,\n",
      "        0.0016, 0.0006, 0.0008, 0.0010, 0.0022, 0.0038, 0.0006, 0.0007, 0.0007,\n",
      "        0.0013, 0.0006, 0.0008, 0.0009, 0.0006, 0.0002, 0.0009, 0.0010, 0.0006,\n",
      "        0.0024, 0.0007, 0.0004, 0.0013, 0.0027, 0.0006, 0.0021, 0.0009, 0.0007,\n",
      "        0.0008, 0.0006, 0.0019, 0.0007, 0.0007, 0.0008, 0.0015, 0.0051, 0.0006,\n",
      "        0.0013, 0.0008, 0.0009, 0.0005, 0.0004, 0.0009, 0.0008, 0.0019, 0.0004,\n",
      "        0.0018, 0.0004, 0.0003, 0.0005, 0.0011, 0.0008, 0.0010, 0.0010, 0.0006,\n",
      "        0.0014, 0.0007, 0.0007, 0.0007, 0.0005, 0.0009, 0.0007, 0.0002, 0.0004,\n",
      "        0.0003, 0.0012, 0.0003, 0.0013, 0.0007, 0.0013, 0.0020, 0.0019, 0.0006,\n",
      "        0.0021, 0.0020, 0.0012, 0.0003, 0.0007, 0.0009, 0.0012, 0.0006, 0.0005,\n",
      "        0.0006, 0.0004, 0.0014, 0.0007, 0.0018, 0.0007, 0.0006, 0.0007, 0.0007,\n",
      "        0.0009, 0.0013, 0.0011, 0.0013, 0.0008, 0.0006, 0.0034, 0.0008, 0.0018,\n",
      "        0.0009, 0.0004, 0.0029, 0.0003, 0.0012, 0.0007, 0.0008, 0.0005, 0.0012,\n",
      "        0.0008, 0.0008, 0.0009, 0.0030, 0.0007, 0.0014, 0.0005, 0.0018, 0.0014,\n",
      "        0.0008, 0.0008, 0.0003, 0.0011, 0.0014, 0.0008, 0.0013, 0.0029, 0.0008,\n",
      "        0.0007, 0.0010, 0.0006, 0.0010, 0.0006, 0.0038, 0.0005, 0.0018, 0.0011,\n",
      "        0.0002, 0.0007, 0.0004, 0.0009, 0.0010, 0.0007, 0.0005, 0.0016, 0.0007,\n",
      "        0.0010, 0.0004, 0.0006, 0.0004, 0.0011, 0.0005, 0.0003, 0.0005, 0.0009,\n",
      "        0.0017, 0.0003, 0.0014, 0.0009, 0.0024, 0.0005, 0.0015, 0.0013, 0.0009,\n",
      "        0.0008, 0.0004, 0.0016, 0.0007, 0.0004, 0.0007, 0.0005, 0.0012, 0.0006,\n",
      "        0.0006, 0.0005, 0.0014, 0.0010, 0.0009, 0.0032, 0.0010, 0.0012, 0.0009,\n",
      "        0.0010, 0.0011, 0.0002, 0.0007, 0.0007, 0.0012, 0.0003, 0.0011, 0.0024,\n",
      "        0.0006, 0.0010, 0.0004, 0.0007, 0.0010, 0.0010, 0.0009, 0.0006, 0.0019,\n",
      "        0.0009, 0.0006, 0.0005, 0.0004, 0.0022, 0.0009, 0.0013, 0.0004, 0.0024,\n",
      "        0.0004, 0.0016, 0.0014, 0.0005, 0.0003, 0.0007, 0.0017, 0.0008, 0.0013,\n",
      "        0.0020, 0.0018, 0.0007, 0.0008, 0.0017, 0.0005, 0.0004, 0.0012, 0.0008,\n",
      "        0.0006, 0.0007, 0.0010, 0.0012, 0.0003, 0.0006, 0.0003, 0.0020, 0.0013,\n",
      "        0.0017, 0.0012, 0.0003, 0.0022, 0.0013, 0.0014, 0.0010, 0.0006, 0.0004,\n",
      "        0.0015, 0.0009, 0.0007, 0.0013, 0.0013, 0.0004, 0.0012, 0.0016, 0.0015,\n",
      "        0.0009, 0.0023, 0.0002, 0.0011, 0.0009, 0.0009, 0.0003, 0.0003, 0.0011,\n",
      "        0.0005, 0.0012, 0.0009, 0.0009, 0.0006, 0.0007, 0.0005, 0.0005, 0.0013,\n",
      "        0.0011, 0.0003, 0.0010, 0.0014, 0.0009, 0.0004, 0.0007, 0.0012, 0.0008,\n",
      "        0.0014, 0.0005, 0.0010, 0.0007, 0.0005, 0.0013, 0.0003, 0.0009, 0.0018,\n",
      "        0.0015, 0.0031, 0.0006, 0.0009, 0.0015, 0.0005, 0.0025, 0.0009, 0.0011,\n",
      "        0.0009, 0.0017, 0.0007, 0.0007, 0.0028, 0.0006, 0.0031, 0.0005, 0.0014,\n",
      "        0.0017, 0.0007, 0.0005, 0.0006, 0.0006, 0.0010, 0.0004, 0.0017, 0.0008,\n",
      "        0.0009, 0.0004, 0.0018, 0.0007, 0.0013, 0.0005, 0.0001, 0.0003, 0.0019,\n",
      "        0.0003, 0.0004, 0.0006, 0.0007, 0.0007, 0.0003, 0.0033, 0.0008, 0.0005,\n",
      "        0.0027, 0.0013, 0.0008, 0.0004, 0.0003, 0.0046, 0.0014, 0.0006, 0.0012,\n",
      "        0.0006, 0.0016, 0.0017, 0.0014, 0.0005, 0.0010, 0.0011, 0.0004, 0.0018,\n",
      "        0.0004, 0.0005, 0.0014, 0.0010, 0.0014, 0.0010, 0.0021, 0.0010, 0.0010,\n",
      "        0.0008, 0.0004, 0.0010, 0.0013, 0.0013, 0.0006, 0.0006, 0.0003, 0.0005,\n",
      "        0.0002, 0.0010, 0.0007, 0.0013, 0.0012, 0.0004, 0.0006, 0.0013, 0.0007,\n",
      "        0.0002, 0.0010, 0.0007, 0.0011, 0.0016, 0.0008, 0.0006, 0.0007, 0.0005,\n",
      "        0.0007, 0.0014, 0.0005, 0.0008, 0.0006, 0.0003, 0.0008, 0.0011, 0.0013,\n",
      "        0.0009, 0.0020, 0.0011, 0.0005, 0.0011, 0.0010, 0.0008, 0.0015, 0.0013,\n",
      "        0.0009, 0.0005, 0.0009, 0.0006, 0.0002, 0.0008, 0.0010, 0.0009, 0.0010,\n",
      "        0.0004, 0.0019, 0.0005, 0.0008, 0.0011, 0.0003, 0.0014, 0.0007, 0.0010,\n",
      "        0.0016, 0.0015, 0.0012, 0.0007, 0.0011, 0.0021, 0.0007, 0.0004, 0.0011,\n",
      "        0.0007, 0.0008, 0.0009, 0.0014, 0.0005, 0.0007, 0.0019, 0.0004, 0.0006,\n",
      "        0.0012, 0.0014, 0.0008, 0.0013, 0.0004, 0.0015, 0.0018, 0.0004, 0.0008,\n",
      "        0.0006], grad_fn=<UnbindBackward0>)\n",
      "torch.Size([1, 1000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "2f49df70a879ebc3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
