## LSTM
LSTM（Long Short-Term Memory）是一种特殊的循环神经网络（RNN），擅长处理和预测序列数据中的长期依赖关系。RNN在处理序列数据（如时间序列或文本）时具有优势，但普通的RNN容易出现梯度消失或爆炸问题，使其难以捕捉较长的上下文信息。LSTM通过设计“记忆单元”解决了这个问题。

LSTM的关键在于它的结构，它引入了三个“门”：
- 遗忘门(Ft)：决定遗忘多少过去的信息(Ct)。
- 输入门(It)：选择性的记忆当前输入的信息。 (采用当前多少数据)
- 输出门(Ot)：决定哪些信息会被输出并传递到下一时刻。

候选记忆:指的是通过当前输入和前一时刻的隐状态生成的一个临时信息候选值，它表示网络“可能”会加入到记忆单元（即长时记忆）中的新信息

长短期记忆的短期神经网络 架构:


![](https://image.chiullian.cn/img/202410291201284.png)

其中

![](https://image.chiullian.cn/img/202410291202417.png)


整体架构:

![](https://image.chiullian.cn/img/202410291256779.png)

在LSTM中，记忆单元 Ct 和隐藏状态 Ht 是两个相互关联但各自独立的状态变量，它们的区别如下：

记忆单元 Ct：
- 这是LSTM中的长期记忆，负责存储在较长时间跨度上保留的信息。
- 记忆单元会通过遗忘门和输入门不断更新，控制哪些信息应该保持，哪些应该丢弃。
- 因为LSTM的设计允许 Ct 在多个时间步之间传播，它能够在序列数据中有效地保留重要的上下文信息，不容易被“遗忘”。
- Ct 通常不会直接参与输出，而是通过控制哪些信息保存在内部。

![](https://image.chiullian.cn/img/202410291400711.png)

![](https://image.chiullian.cn/img/202410291402983.png)

隐藏状态 Ht：
- 这是LSTM的短期记忆，代表当前时间步的输出，同时会传递给下一个时间步。
- 隐藏状态会经过输出门的调节，把一部分记忆信息 Ct 提取出来并用于当前时间步的输出。
- Ht 被认为是LSTM在当前时间步的即时状态，因此会在每个时间步更新。
- Ht 也会在模型训练时直接用于计算损失，从而指导模型的学习。


![](https://image.chiullian.cn/img/202410291359293.png)