# 图神经网络

点的信息是这样传递的
![](https://image.chiullian.cn/img/202411051954533.png)

#### 首先是图的介绍
![](https://image.chiullian.cn/img/202411051952007.png)

* Vertex. 每一个点可以是一个向量可以使一个标量,表示这个点的状态,比如说人的体重身高手机号等等,
* Edge. 每一条边可以理解成两个点之间的关系向量,
* U. 全局信息的一个点或者向量 (像是一条无形的点连着这图上所有的点的边的一个点,包含这图的全局所有信息)

就像下图,边也代表一种状态,人和人之间的关系有看,打,坐等等的关系,点则是人的性质
![](https://image.chiullian.cn/img/202411052023560.png)

层之间的输入是一张图输出也是一张图, 大小是不会变的

Images as graphs
下面是把图片映射成图的样子

![](https://image.chiullian.cn/img/202411052021847.png)

但是邻接表的范围还是太大了, 所以这里采用存边的方法,记录边的两头连着那两个节点


![](https://image.chiullian.cn/img/202411052030768.png)

MLP

GNN 的消息传递神经网络去更新图, GNN采用“graph-in，graph-out”架构, 不会改变其联通性

那么两个layer层之间的信息是怎么传递的呢, 所有点Un通过一个共用的MLP层或者其他可微层,得到Un+1
所有边En通过共用一个MLP层进行学习到下一个点En+1

可能在图中的边中存储了信息，但在节点中没有信息，但仍然需要对节点进行预测。我们需要一种方法来从边缘收集信息，并将其提供给节点进行预测。我们可以通过合并来实现这一点。

具体如何池化如下图, 这里是直接相加
![](https://image.chiullian.cn/img/202411052121507.png)

#### 在图的各个部分之间传递消息
对于图中的每个节点，收集所有相邻节点嵌入（或消息）, 通过聚合函数（如sum,平均等等）聚合所有消息。所有池化的消息都通过一个更新函数传递，通常是一个学习过的神经网络。

***这些步骤是利用图的连通性的关键。我们将在GNN层中构建更精细的消息传递变体，从而产生具有更高表现力和功能的GNN模型。***

如何在传递是结合点边全局的示例图:

![](https://image.chiullian.cn/img/202411052130994.png)


#### 最后
调参: 层数, Embedding有多大, 汇聚用什么操作, 怎么做信息的传递.